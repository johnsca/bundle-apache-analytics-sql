# Apache Analytics with SQL

This bundle is an 8 node cluster designed to scale out. Built around Apache
Hadoop components and MySQL, it contains the following units:

* 1 HDFS Master
* 1 HDFS Secondary Namenode
* 1 YARN Master
* 3 Compute Slaves
* 1 Hive
  - 1 Plugin (colocated on the Hive unit)
* 1 MySQL

## Usage
Deploy this bundle using juju-quickstart:

    juju quickstart <bundle>

See `juju quickstart --help` for ways to specify the `<bundle>` argument.
Alternatively, you may deploy this bundle using juju-deployer:

    juju deployer -c </path/to/bundles.yaml> apache-analytics-sql

### Smoke test HDFS admin functionality
Once the deployment is complete and the cluster is running, ssh to the HDFS
Master unit:

    juju ssh hdfs-master/0

As the `ubuntu` user, create a temporary directory on the Hadoop file system.
The steps below verify HDFS functionality:

    hdfs dfs -mkdir -p /tmp/hdfs-test
    hdfs dfs -chmod -R 777 /tmp/hdfs-test
    hdfs dfs -ls /tmp # verify the newly created hdfs-test subdirectory exists
    hdfs dfs -rm -R /tmp/hdfs-test
    hdfs dfs -ls /tmp # verify the hdfs-test subdirectory has been removed
    exit

### Smoke test YARN and MapReduce
Run the `terasort.sh` script from the Hive unit to generate and sort data. The
steps below verify that Hive is communicating with the cluster via the plugin
and that YARN and MapReduce are working as expected:

    juju ssh hive/0
    ~/terasort.sh
    exit

### Smoke test HDFS functionality from user space
From the Hive unit, delete the MapReduce output previously generated by the
`terasort.sh` script:

    juju ssh hive/0
    hdfs dfs -rm -R /user/ubuntu/tera_demo_out
    exit

### HIVE + HDFS Usage:

From the Hive unit, switch to the Hive user, and start the Hive console:

    juju ssh hive/0
    sudo su hive
    hive

From the Hive console, create a table:

    show databases;
    create table test(col1 int, col2 string);
    show tables;
    exit;

Exit from the Hive user session:

    exit

As the `ubuntu` user, verify connection to the HDFS cluster, and verify a
`test` subdirectory has been created on the remote HDFS cluster:

    hdfs dfsadmin -report
    hdfs dfs -ls /user/hive/warehouse
    exit

## Scale Out Usage
This bundle was designed to scale out. To increase the amount of Compute
Slaves, you can add units to the compute-slave service. To add one unit:

    juju add-unit compute-slave

Or you can add multiple units at once:

    juju add-unit -n4 compute-slave

## Contact Information

- [bigdata-dev@canonical.com](mailto:bigdata-dev@canonical.com)

## Help

- [Hive bug tracker](https://issues.apache.org/jira/browse/HIVE)
- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)
- [Juju community](https://jujucharms.com/community)
