# Apache Analytics with SQL

This bundle is a 7 node cluster designed to scale out. Built around Apache
Hadoop components and MySQL, it contains the following units:

* 1 HDFS Master
* 1 YARN Master
* 3 Compute Slaves
* 1 Hadoop Client
  - 1 Hive (colocated on the Hadoop Client unit)
* 1 MySQL

## Usage
Deploy this bundle using juju-quickstart:

    juju quickstart <bundle>

See `juju quickstart --help` for ways to specify the `<bundle>` argument.
Alternatively, you may deploy this bundle using juju-deployer:

    juju deployer -c </path/to/bundles.yaml> apache-analytics-sql

### Smoke test HDFS admin functionality
Once the deployment is complete and the cluster is running, ssh to the HDFS
Master unit:

    juju ssh hdfs-master/0

As the HDFS user, create a /user/ubuntu directory on the Hadoop file system.
The steps below verify HDFS functionality:

    sudo su $HDFS_USER
    hdfs dfs -mkdir -p /user/ubuntu
    hdfs dfs -chown ubuntu:ubuntu /user/ubuntu
    hdfs dfs -chmod -R 755 /user/ubuntu
    exit

### Smoke test YARN and MapReduce
Run the Terasort script from the Hadoop Client unit to generate and sort data.
The steps below verify YARN and MapReduce functionality:

    juju ssh client/0
    ~/terasort.sh
    exit

### Smoke test HDFS functionality from user space
From the Hadoop Client unit, delete the MapReduce output previously generated by
the Terasort script:

    juju ssh client/0
    hdfs dfs -rm -r /user/ubuntu/out_dir
    exit

### HIVE + HDFS Usage:

From the Hive unit, switch to the Hive user, and start the Hive console:

    juju ssh hive/0
    sudo su hive
    hive

From the Hive console, create a table:

    show databases;
    create table test(col1 int, col2 string);
    show tables;
    exit;

Exit from the Hive user session:

    exit

Change to the HDFS user, verify connection to the HDFS cluster, and verify a
test directory has been created on the remote HDFS cluster:

    sudo su hdfs
    hdfs dfsadmin -report
    hdfs dfs -ls /user/hive/warehouse
    exit

## Scale Out Usage
This bundle was designed to scale out. To increase the amount of Compute
Slaves, you can add units to the compute-slave service. To add one unit:

    juju add-unit compute-slave

Or you can add multiple units at once:

    juju add-unit -n4 compute-slave

## Contact Information
* [Amir Sanjar](<amir.sanjar@canonical.com>)
* [Cory Johns](<cory.johns@canonical.com>)
* [Kevin Monroe](kevin.monroe@canonical.com>)

## Help

- [Hive bug tracker](https://issues.apache.org/jira/browse/HIVE)
- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)
- [Juju community](https://jujucharms.com/community)
